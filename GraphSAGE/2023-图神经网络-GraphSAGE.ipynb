{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"18608840882D48C99AF17747811DF15C","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["\n","\n","## GraphSAGE 核心思想\n","\n","GraphSAGE的核心：GraphSAGE不是试图学习一个图上所有node的embedding，而是学习一个为每个node产生embedding的映射。\n","\n","\n","论文中提出的方法称为GraphSAGE, SAGE指的是 Sample and Aggregate，不是对每个顶点都训练一个单独的embeddding向量，而是训练了一组aggregator functions，这些函数学习如何从一个顶点的局部邻居聚合特征信息。每个聚合函数从一个顶点的不同的hops或者说不同的搜索深度聚合信息。测试或是推断的时候，使用训练好的系统，通过学习到的聚合函数来对完全未见过的顶点生成embedding。\n","\n","![](https://img-blog.csdnimg.cn/img_convert/beebbdcaf3b468efee9b9ffdb530c3dd.png)\n","上面是为红色的目标节点生成embedding的过程。k表示距离目标节点的搜索深度，k=1就是目标节点的相邻节点，k=2表示目标节点相邻节点的相邻节点。\n","对于上图中的例子：\n","- 第一步是采样，k=1采样了3个节点，对k=2采用了5个节点；\n","- 第二步是聚合邻居节点的信息，获得目标节点的embedding；\n","- 第三步是使用聚合得到的信息，也就是目标节点的embedding,来预测图中想预测的信息;\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["此课件来自如下链接的整合\n","\n","https://zhuanlan.zhihu.com/p/410407148\n","\n","https://zhuanlan.zhihu.com/p/512929377\n","\n","https://blog.csdn.net/weixin_44027006/article/details/116888648\n","\n","https://www.heywhale.com/mw/project/608538b1c7cba5001752d619"]},{"cell_type":"markdown","metadata":{},"source":["## graphSAGE 源码"]},{"cell_type":"markdown","metadata":{},"source":["### 采样"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import numpy as np\n","\n","def sampling(src_nodes, sample_num, neighbor_table):\n","    \"\"\"根据源节点采样指定数量的邻居节点，注意使用的是有放回的采样；\n","    某个节点的邻居节点数量少于采样数量时，采样结果出现重复的节点\n","    Arguments:\n","        src_nodes {list, ndarray} -- 源节点列表\n","        sample_num {int} -- 需要采样的节点数\n","        neighbor_table {dict} -- 节点到其邻居节点的映射表\n","    Returns:\n","        np.ndarray -- 采样结果构成的列表\n","    \"\"\"\n","    results = []\n","    for sid in src_nodes:\n","        # # 从节点的邻居中进行有放回地进行采样\n","        # res = np.random.choice(neighbor_table[sid], size=(sample_num,))\n","        # results.append(res)\n","        if len(neighbor_table[sid]) >= sample_num:\n","            res = np.random.choice(neighbor_table[sid], size=(sample_num,),replace=False)\n","        else:\n","            res = np.random.choice(neighbor_table[sid], size=(sample_num,),replace=True)\n","        results.append(res)\n","    return np.asarray(results).flatten()\n","\n","\n","def multihop_sampling(src_nodes, sample_nums, neighbor_table):\n","    \"\"\"根据源节点进行多阶采样\n","    Arguments:\n","        src_nodes {list, np.ndarray} -- 源节点id\n","        sample_nums {list of int} -- 每一阶需要采样的个数\n","        neighbor_table {dict} -- 节点到其邻居节点的映射\n","    Returns:\n","        [list of ndarray] -- 每一阶采样的结果\n","    \"\"\"\n","    sampling_result = [src_nodes]\n","    # print(\"sampling result = \", sampling_result)\n","    # print(\"sample_nums = \", sample_nums)\n","    for k, hopk_num in enumerate(sample_nums):\n","        # print(\"sampling_result[k] = \", sampling_result[k])\n","        hopk_result = sampling(sampling_result[k], hopk_num, neighbor_table)\n","        sampling_result.append(hopk_result)\n","    return sampling_result\n"]},{"cell_type":"markdown","metadata":{},"source":["### 聚合与训练"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","\n","\n","class NeighborAggregator(nn.Module):\n","    def __init__(self, input_dim, output_dim,\n","                    use_bias=False, aggr_method=\"mean\"):\n","        \"\"\"聚合节点邻居\n","        Args:\n","            input_dim: 输入特征的维度\n","            output_dim: 输出特征的维度\n","            use_bias: 是否使用偏置 (default: {False})\n","            aggr_method: 邻居聚合方式 (default: {mean})\n","        \"\"\"\n","        super(NeighborAggregator, self).__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.use_bias = use_bias\n","        self.aggr_method = aggr_method\n","        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n","        if self.use_bias:\n","            self.bias = nn.Parameter(torch.Tensor(self.output_dim))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        init.kaiming_uniform_(self.weight)\n","        if self.use_bias:\n","            init.zeros_(self.bias)\n","\n","    def forward(self, neighbor_feature):\n","        if self.aggr_method == \"mean\":\n","            aggr_neighbor = neighbor_feature.mean(dim=1)\n","        elif self.aggr_method == \"sum\":\n","            aggr_neighbor = neighbor_feature.sum(dim=1)\n","        elif self.aggr_method == \"max\":\n","            aggr_neighbor = neighbor_feature.max(dim=1)\n","        else:\n","            raise ValueError(\"Unknown aggr type, expected sum, max, or mean, but got {}\"\n","                                .format(self.aggr_method))\n","\n","        neighbor_hidden = torch.matmul(aggr_neighbor, self.weight)\n","        if self.use_bias:\n","            neighbor_hidden += self.bias\n","\n","        return neighbor_hidden\n","\n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, aggr_method={}'.format(\n","            self.input_dim, self.output_dim, self.aggr_method)\n","\n","\n","class SageGCN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim,\n","                    activation=F.relu,\n","                    aggr_neighbor_method=\"mean\",\n","                    aggr_hidden_method=\"sum\"):\n","        \"\"\"SageGCN层定义\n","\n","        Args:\n","            input_dim: 输入特征的维度\n","            hidden_dim: 隐层特征的维度，\n","                当aggr_hidden_method=sum, 输出维度为hidden_dim\n","                当aggr_hidden_method=concat, 输出维度为hidden_dim*2\n","            activation: 激活函数\n","            aggr_neighbor_method: 邻居特征聚合方法，[\"mean\", \"sum\", \"max\"]\n","            aggr_hidden_method: 节点特征的更新方法，[\"sum\", \"concat\"]\n","        \"\"\"\n","        super(SageGCN, self).__init__()\n","        assert aggr_neighbor_method in [\"mean\", \"sum\", \"max\"]\n","        assert aggr_hidden_method in [\"sum\", \"concat\"]\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.aggr_neighbor_method = aggr_neighbor_method\n","        self.aggr_hidden_method = aggr_hidden_method\n","        self.activation = activation\n","        self.aggregator = NeighborAggregator(input_dim, hidden_dim,\n","                                                aggr_method=aggr_neighbor_method)\n","        self.weight = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        init.kaiming_uniform_(self.weight)\n","\n","    def forward(self, src_node_features, neighbor_node_features):\n","        neighbor_hidden = self.aggregator(neighbor_node_features)\n","        self_hidden = torch.matmul(src_node_features, self.weight)\n","\n","        if self.aggr_hidden_method == \"sum\":\n","            hidden = self_hidden + neighbor_hidden\n","        elif self.aggr_hidden_method == \"concat\":\n","            hidden = torch.cat([self_hidden, neighbor_hidden], dim=1)\n","        else:\n","            raise ValueError(\"Expected sum or concat, got {}\"\n","                                .format(self.aggr_hidden))\n","        if self.activation:\n","            return self.activation(hidden)\n","        else:\n","            return hidden\n","\n","    def extra_repr(self):\n","        output_dim = self.hidden_dim if self.aggr_hidden_method == \"sum\" else self.hidden_dim * 2\n","        return 'in_features={}, out_features={}, aggr_hidden_method={}'.format(\n","            self.input_dim, output_dim, self.aggr_hidden_method)\n","\n","\n","class GraphSage(nn.Module):\n","    def __init__(self, input_dim, hidden_dim,\n","                    num_neighbors_list):\n","        super(GraphSage, self).__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_neighbors_list = num_neighbors_list\n","        self.num_layers = len(num_neighbors_list)\n","        self.gcn = nn.ModuleList()\n","        self.gcn.append(SageGCN(input_dim, hidden_dim[0]))\n","        for index in range(0, len(hidden_dim) - 2):\n","            self.gcn.append(SageGCN(hidden_dim[index], hidden_dim[index + 1]))\n","        self.gcn.append(SageGCN(hidden_dim[-2], hidden_dim[-1], activation=None))\n","\n","    def forward(self, node_features_list):\n","        hidden = node_features_list\n","\n","        for l in range(self.num_layers):\n","            # print(f\"========= 第 {l} 层 =========\")\n","            next_hidden = []\n","            gcn = self.gcn[l]\n","            for hop in range(self.num_layers - l):\n","            # print(\"self.num_layers - l \" , self.num_layers - l-1)\n","            # for hop in range(self.num_layers - l-1,l,-1):\n","            #     print(f\"======== hop {hop} ============ \" )\n","                src_node_features = hidden[hop]\n","                src_node_num = len(src_node_features)\n","                # print(\" src_node_num = \", src_node_features.shape)\n","\n","                neighbor_node_features = hidden[hop + 1].view((src_node_num, self.num_neighbors_list[hop], -1))\n","                # print(\" neighbor_node_features = \", neighbor_node_features.shape)\n","\n","                h = gcn(src_node_features, neighbor_node_features)\n","                # print(\" after gcn h = \", h.shape)\n","\n","                next_hidden.append(h)\n","            hidden = next_hidden\n","            # print(\"hidden shape = \",len(hidden))\n","        return hidden[0]\n","\n","    def extra_repr(self):\n","        return 'in_features={}, num_neighbors_list={}'.format(\n","            self.input_dim, self.num_neighbors_list\n","        )\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 基于pyg的graphsage实现"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os.path as osp\n","import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.datasets import Planetoid\n","from tqdm import tqdm\n","\n","from torch_geometric.loader import NeighborSampler\n","from torch_geometric.nn import SAGEConv"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["dataset =Planetoid(root=r\"./data\",name='Cora')\n","data = dataset[0]\n","num_nodes_list = torch.arange(data.num_nodes)\n","train_idx = num_nodes_list[data['train_mask']]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["140"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["len(train_idx)"]},{"cell_type":"markdown","metadata":{},"source":["### NeighborSampler\n","\n","https://blog.csdn.net/qq_40671063/article/details/126803861"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n","                               sizes=[15, 10, 5], batch_size=70,\n","                               shuffle=True, num_workers=12)\n","subgraph_loader = NeighborSampler(data.edge_index, node_idx=None, sizes=[-1],\n","                                  batch_size=256, shuffle=False,\n","                                  num_workers=12)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["2"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["len(train_loader)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[EdgeIndex(edge_index=tensor([[  70,   71,   72,  ...,  701,  786, 1460],\n","        [   0,    0,    0,  ...,  900,  900,  900]]), e_id=tensor([10054,  6387,  5454,  ...,  7858,   264,  9870]), size=(1461, 901)), EdgeIndex(edge_index=tensor([[ 70,  71,  72,  ..., 900,  69, 643],\n","        [  0,   0,   0,  ..., 346, 347, 347]]), e_id=tensor([10054,  6387,  5454,  ...,  7839,    69,  9498]), size=(901, 348)), EdgeIndex(edge_index=tensor([[ 70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n","          84,  16,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,\n","          97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,\n","         111, 112, 113, 114,  13, 115, 116, 117, 118, 119,  11, 118, 120, 121,\n","         122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135,\n","         136, 137,   5, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148,\n","         149, 150,  42, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161,\n","         162,  74, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174,\n","         175, 176, 177, 178, 179,  73, 166, 180, 181,  36, 182, 183, 184, 154,\n","         185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n","         199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n","         213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,  29,\n","          43, 182, 183, 184, 226,  74, 166, 227, 228, 229, 230, 231, 232, 233,\n","         118, 154, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245,\n","         246, 247, 248, 249, 250, 251, 252, 253,  20, 151, 254, 255,  36, 182,\n","         184, 226, 118, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 120,\n","         266, 267, 268, 269, 270, 271, 272, 273,  73, 274, 275, 276, 277, 278,\n","         279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292,\n","         293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306,\n","         307, 308, 309, 310, 311, 312, 313, 314, 315, 206, 209, 213, 245, 261,\n","         316, 317, 318, 319, 320,   5,  86, 321, 322, 323, 324, 325, 249, 326,\n","         327, 328, 329,  73, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n","         154, 340, 341, 342, 343, 344, 345, 346, 347],\n","        [  0,   0,   0,   0,   0,   1,   1,   1,   1,   2,   2,   3,   4,   4,\n","           4,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n","           5,   5,   6,   6,   6,   6,   6,   7,   7,   7,   7,   7,   8,   8,\n","           8,   9,   9,  10,  11,  11,  11,  12,  12,  12,  13,  13,  13,  13,\n","          13,  14,  14,  14,  14,  14,  14,  15,  15,  15,  15,  15,  15,  15,\n","          15,  15,  16,  16,  16,  17,  17,  17,  18,  18,  18,  18,  18,  19,\n","          19,  19,  20,  20,  20,  20,  20,  20,  20,  20,  20,  20,  21,  22,\n","          22,  23,  23,  23,  23,  23,  23,  23,  23,  24,  24,  24,  25,  25,\n","          26,  27,  27,  27,  27,  28,  28,  28,  28,  29,  29,  29,  29,  30,\n","          30,  30,  30,  30,  30,  30,  30,  30,  30,  30,  30,  31,  31,  32,\n","          32,  32,  32,  32,  32,  33,  33,  34,  34,  34,  34,  34,  34,  34,\n","          34,  34,  34,  34,  34,  35,  35,  35,  35,  35,  35,  35,  35,  36,\n","          36,  36,  36,  36,  36,  37,  37,  37,  37,  37,  37,  37,  37,  37,\n","          38,  38,  38,  39,  39,  39,  39,  39,  40,  40,  40,  40,  40,  41,\n","          41,  41,  41,  41,  41,  41,  41,  41,  42,  42,  42,  42,  43,  43,\n","          43,  43,  44,  44,  44,  44,  44,  44,  45,  46,  46,  46,  46,  47,\n","          47,  47,  47,  47,  47,  47,  47,  47,  48,  48,  48,  49,  49,  50,\n","          50,  50,  50,  51,  51,  51,  51,  51,  52,  52,  52,  52,  53,  53,\n","          54,  54,  54,  54,  54,  55,  55,  55,  55,  55,  55,  56,  56,  56,\n","          56,  56,  56,  57,  57,  58,  58,  58,  58,  59,  59,  59,  59,  59,\n","          59,  59,  59,  59,  59,  60,  60,  60,  61,  61,  61,  61,  62,  62,\n","          62,  62,  62,  63,  63,  63,  63,  63,  63,  64,  64,  65,  65,  66,\n","          67,  67,  67,  67,  67,  67,  68,  69,  69]]), e_id=tensor([10054,  6387,  5454,  1214,   477,  9678,  4945,  4251,  4238, 10223,\n","         1954, 10388, 10541, 10379,  6256,   599,  5536,  9003,  2050,  8296,\n","         5007,  7518,  7017,  6959,  1675,  8290,  4596,  2780,  8299,  8292,\n","         8846,  5112,  5110,  3638,    77,  9390,  6307,  4913,  4541,  2311,\n","         9135,  8923,  6526,  7311,  2663,  1646,     6,  2643,  2640, 10527,\n","         8135,   880,     3,  8132,  6597,  5774,  1382,  9814,  9809,  6544,\n","         6059,  3735,   684,  8752,  8747,  8745,  8423,  4092,  3606,  2786,\n","         2397,  1157,   338,  8305,  8119,  7646,  3024,  2214,  9246,  6813,\n","         6803,  4816,   139,  8952,  3354,   153,   464, 10422,  6920,  6454,\n","         5261,  5003,  4136,  2518,  2496,  2363,  3784, 10363,  2878,   476,\n","         9326,  7624,  7601,  6391,  6193,  5758,  4867, 10306,  7565,  2569,\n","         7339,  6729,  6876,  8890,  5924,  3691,  2252,  1213,  6392,  8598,\n","         2005,   436,  9985,  9982,    89,  5264,  7072,  7047,  6910,  4726,\n","         4647,  4446,  4010,  3479,  3194,  2274,  1845, 10304,  6014,  9200,\n","         9192,  9188,  9184,  2381,   322,  9663,  1889,  8320,  8318,  8121,\n","         4515,  4202,  3250,  3155,  3114,  2633,  1353,   885,   231,  8695,\n","         8693,  8652,  7218,  6508,  6158,  5959,  4474,   572,   576,  9984,\n","         9981,    88, 10347,   478,  6394,  8611,  8439,  7233,  7196,  6612,\n","         1854,  1614,  8134,  5262,  1587,  8990,  8985,  7110,  4881,  3963,\n","         9825,  7496,  3256,  2120,  1986,  9864,  8902,  8486,  6002,  5633,\n","         4359,  4039,  2391,  2230,   220, 10423, 10046,  6899,   437,  9986,\n","           90, 10348,  8133, 10457, 10450,  9469,  4746,  2913,  9857, 10442,\n","        10440,  5127,  3877,  6598, 10032,  9128,  9125,  8427,  6582,  4002,\n","         2850,  2430,  1216,  7184,   976,  6488,  4451,  7528,  6361,  1837,\n","         1670,  9023,  8977,  8490,  5812,  4454,  7789,  2599,  2421,   779,\n","         9675,  6326,  8019,  3979,  3581,  1965,  1546,  9724,  8765,  8755,\n","         8242,  4069,  1377,  9152,  9149,  9146,  5532,  2833,  1885,  7482,\n","         6553,  6532,  4342,  3778,  2218,  8321,  4516,  3115,  9863,  9858,\n","         6458,  5020,  2528,  2228,   987,   337,  9002,  8309, 10402,  9926,\n","         6483,  3773,  5632,  9506,  8943,  5078,  4206,  1211,  7260,  7179,\n","         2491,  1990,   631,  8969,  3322,  5247,  2668,  5716,  5267, 10406,\n","        10085,  7117,  4556,  3073,  5137,  9502,  4061]), size=(348, 70))]\n","70\n","tensor([ 124,   15,   10,  ...,   64, 1727, 2422])\n"]}],"source":["for batch_size, n_id, adjs in train_loader:\n","    print(adjs)\n","    print(batch_size)\n","    print(n_id)\n","    break"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["1461"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["len(n_id)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([ 124,   15,   10,  ...,   64, 1727, 2422])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["n_id"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(124)\n","tensor(15)\n","tensor(10)\n","tensor(66)\n","tensor(44)\n","tensor(88)\n","tensor(17)\n","tensor(110)\n","tensor(108)\n","tensor(77)\n","tensor(134)\n","tensor(1)\n","tensor(71)\n","tensor(2)\n","tensor(43)\n","tensor(33)\n","tensor(130)\n","tensor(113)\n","tensor(22)\n","tensor(87)\n","tensor(59)\n","tensor(115)\n","tensor(9)\n","tensor(102)\n","tensor(0)\n","tensor(13)\n","tensor(58)\n","tensor(93)\n","tensor(112)\n","tensor(122)\n","tensor(73)\n","tensor(83)\n","tensor(75)\n","tensor(125)\n","tensor(55)\n","tensor(36)\n","tensor(99)\n","tensor(133)\n","tensor(68)\n","tensor(4)\n","tensor(120)\n","tensor(74)\n","tensor(105)\n","tensor(123)\n","tensor(45)\n","tensor(57)\n","tensor(12)\n","tensor(48)\n","tensor(138)\n","tensor(82)\n","tensor(56)\n","tensor(52)\n","tensor(41)\n","tensor(46)\n","tensor(132)\n","tensor(91)\n","tensor(51)\n","tensor(11)\n","tensor(107)\n","tensor(65)\n","tensor(76)\n","tensor(16)\n","tensor(61)\n","tensor(103)\n","tensor(131)\n","tensor(97)\n","tensor(50)\n","tensor(111)\n","tensor(63)\n","tensor(21)\n","tensor(2478)\n","tensor(1622)\n","tensor(1367)\n","tensor(306)\n","tensor(109)\n","tensor(2367)\n","tensor(1271)\n","tensor(1093)\n","tensor(1090)\n","tensor(2545)\n","tensor(476)\n","tensor(2631)\n","tensor(2701)\n","tensor(2624)\n","tensor(1582)\n","tensor(1394)\n","tensor(2178)\n","tensor(498)\n","tensor(2015)\n","tensor(1288)\n","tensor(1847)\n","tensor(1741)\n","tensor(1732)\n","tensor(415)\n","tensor(2012)\n","tensor(1174)\n","tensor(696)\n","tensor(2016)\n","tensor(2013)\n","tensor(2140)\n","tensor(1316)\n","tensor(1315)\n","tensor(927)\n","tensor(24)\n","tensor(2279)\n","tensor(1599)\n","tensor(1262)\n","tensor(1161)\n","tensor(567)\n","tensor(2209)\n","tensor(2157)\n","tensor(1647)\n","tensor(1803)\n","tensor(659)\n","tensor(406)\n","tensor(654)\n","tensor(652)\n","tensor(2691)\n","tensor(1986)\n","tensor(206)\n","tensor(1666)\n","tensor(1454)\n","tensor(332)\n","tensor(2400)\n","tensor(2399)\n","tensor(1653)\n","tensor(1530)\n","tensor(963)\n","tensor(152)\n","tensor(2121)\n","tensor(2120)\n","tensor(2119)\n","tensor(2040)\n","tensor(1051)\n","tensor(911)\n","tensor(698)\n","tensor(588)\n","tensor(286)\n","tensor(2017)\n","tensor(1982)\n","tensor(1884)\n","tensor(747)\n","tensor(540)\n","tensor(2238)\n","tensor(1703)\n","tensor(1702)\n","tensor(1234)\n","tensor(39)\n","tensor(2164)\n","tensor(842)\n","tensor(42)\n","tensor(2651)\n","tensor(1725)\n","tensor(1627)\n","tensor(1358)\n","tensor(1287)\n","tensor(1067)\n","tensor(615)\n","tensor(609)\n","tensor(580)\n","tensor(973)\n","tensor(2614)\n","tensor(723)\n","tensor(2256)\n","tensor(1878)\n","tensor(1871)\n","tensor(1623)\n","tensor(1561)\n","tensor(1448)\n","tensor(1251)\n","tensor(2582)\n","tensor(1862)\n","tensor(633)\n","tensor(1810)\n","tensor(1701)\n","tensor(1715)\n","tensor(2151)\n","tensor(1495)\n","tensor(950)\n","tensor(550)\n","tensor(2080)\n","tensor(487)\n","tensor(2455)\n","tensor(2454)\n","tensor(26)\n","tensor(1751)\n","tensor(1745)\n","tensor(1723)\n","tensor(1214)\n","tensor(1189)\n","tensor(1136)\n","tensor(1035)\n","tensor(876)\n","tensor(797)\n","tensor(558)\n","tensor(449)\n","tensor(2581)\n","tensor(1520)\n","tensor(2225)\n","tensor(2224)\n","tensor(2223)\n","tensor(2222)\n","tensor(583)\n","tensor(84)\n","tensor(2363)\n","tensor(458)\n","tensor(2021)\n","tensor(2020)\n","tensor(1983)\n","tensor(1156)\n","tensor(1079)\n","tensor(815)\n","tensor(787)\n","tensor(771)\n","tensor(651)\n","tensor(323)\n","tensor(210)\n","tensor(60)\n","tensor(2107)\n","tensor(2106)\n","tensor(2094)\n","tensor(1781)\n","tensor(1640)\n","tensor(1552)\n","tensor(1505)\n","tensor(1146)\n","tensor(2604)\n","tensor(2082)\n","tensor(2045)\n","tensor(1784)\n","tensor(1777)\n","tensor(1670)\n","tensor(452)\n","tensor(399)\n","tensor(391)\n","tensor(2176)\n","tensor(2175)\n","tensor(1761)\n","tensor(1256)\n","tensor(1016)\n","tensor(2405)\n","tensor(1842)\n","tensor(816)\n","tensor(514)\n","tensor(483)\n","tensor(2419)\n","tensor(2155)\n","tensor(2052)\n","tensor(1517)\n","tensor(1416)\n","tensor(1118)\n","tensor(1042)\n","tensor(586)\n","tensor(544)\n","tensor(2476)\n","tensor(1721)\n","tensor(2668)\n","tensor(2667)\n","tensor(2303)\n","tensor(1219)\n","tensor(733)\n","tensor(2418)\n","tensor(2662)\n","tensor(2661)\n","tensor(1318)\n","tensor(1001)\n","tensor(2471)\n","tensor(2206)\n","tensor(2205)\n","tensor(2041)\n","tensor(1662)\n","tensor(1031)\n","tensor(714)\n","tensor(598)\n","tensor(1776)\n","tensor(236)\n","tensor(1634)\n","tensor(1138)\n","tensor(1849)\n","tensor(1616)\n","tensor(447)\n","tensor(412)\n","tensor(2182)\n","tensor(2172)\n","tensor(2053)\n","tensor(1467)\n","tensor(1139)\n","tensor(1914)\n","tensor(644)\n","tensor(596)\n","tensor(175)\n","tensor(2366)\n","tensor(1604)\n","tensor(1959)\n","tensor(1022)\n","tensor(904)\n","tensor(479)\n","tensor(379)\n","tensor(2380)\n","tensor(2123)\n","tensor(2122)\n","tensor(2001)\n","tensor(1046)\n","tensor(330)\n","tensor(2215)\n","tensor(2214)\n","tensor(2213)\n","tensor(1392)\n","tensor(710)\n","tensor(457)\n","tensor(1839)\n","tensor(1655)\n","tensor(1650)\n","tensor(1113)\n","tensor(971)\n","tensor(541)\n","tensor(1628)\n","tensor(1293)\n","tensor(619)\n","tensor(543)\n","tensor(239)\n","tensor(2018)\n","tensor(2642)\n","tensor(2444)\n","tensor(1632)\n","tensor(970)\n","tensor(2312)\n","tensor(2162)\n","tensor(1309)\n","tensor(1080)\n","tensor(1790)\n","tensor(1775)\n","tensor(608)\n","tensor(484)\n","tensor(139)\n","tensor(2169)\n","tensor(834)\n","tensor(1353)\n","tensor(661)\n","tensor(1441)\n","tensor(2643)\n","tensor(2492)\n","tensor(1762)\n","tensor(1169)\n","tensor(758)\n","tensor(1322)\n","tensor(2310)\n","tensor(1043)\n","tensor(1713)\n","tensor(162)\n","tensor(300)\n","tensor(2014)\n","tensor(1527)\n","tensor(841)\n","tensor(851)\n","tensor(2425)\n","tensor(1289)\n","tensor(1270)\n","tensor(2143)\n","tensor(1778)\n","tensor(1656)\n","tensor(1584)\n","tensor(1196)\n","tensor(980)\n","tensor(1802)\n","tensor(1805)\n","tensor(1771)\n","tensor(2086)\n","tensor(1782)\n","tensor(1193)\n","tensor(581)\n","tensor(1998)\n","tensor(2093)\n","tensor(153)\n","tensor(318)\n","tensor(563)\n","tensor(1337)\n","tensor(459)\n","tensor(426)\n","tensor(2370)\n","tensor(2177)\n","tensor(1147)\n","tensor(1598)\n","tensor(894)\n","tensor(765)\n","tensor(599)\n","tensor(640)\n","tensor(1800)\n","tensor(1140)\n","tensor(2137)\n","tensor(211)\n","tensor(2024)\n","tensor(1979)\n","tensor(737)\n","tensor(1652)\n","tensor(437)\n","tensor(436)\n","tensor(356)\n","tensor(2071)\n","tensor(1013)\n","tensor(2593)\n","tensor(1566)\n","tensor(1224)\n","tensor(748)\n","tensor(204)\n","tensor(1843)\n","tensor(1850)\n","tensor(818)\n","tensor(272)\n","tensor(2394)\n","tensor(1912)\n","tensor(1570)\n","tensor(571)\n","tensor(277)\n","tensor(2010)\n","tensor(471)\n","tensor(2139)\n","tensor(1679)\n","tensor(1301)\n","tensor(2141)\n","tensor(1636)\n","tensor(201)\n","tensor(2466)\n","tensor(565)\n","tensor(2460)\n","tensor(2441)\n","tensor(1199)\n","tensor(1509)\n","tensor(1072)\n","tensor(564)\n","tensor(189)\n","tensor(1513)\n","tensor(760)\n","tensor(631)\n","tensor(536)\n","tensor(290)\n","tensor(1504)\n","tensor(2160)\n","tensor(2159)\n","tensor(2158)\n","tensor(1835)\n","tensor(354)\n","tensor(2505)\n","tensor(505)\n","tensor(350)\n","tensor(470)\n","tensor(745)\n","tensor(1987)\n","tensor(1704)\n","tensor(366)\n","tensor(1997)\n","tensor(1989)\n","tensor(2002)\n","tensor(899)\n","tensor(151)\n","tensor(2381)\n","tensor(606)\n","tensor(49)\n","tensor(2615)\n","tensor(2003)\n","tensor(665)\n","tensor(2434)\n","tensor(2240)\n","tensor(1443)\n","tensor(1585)\n","tensor(1369)\n","tensor(1094)\n","tensor(89)\n","tensor(2464)\n","tensor(805)\n","tensor(258)\n","tensor(29)\n","tensor(1785)\n","tensor(1417)\n","tensor(1157)\n","tensor(1141)\n","tensor(884)\n","tensor(2637)\n","tensor(442)\n","tensor(218)\n","tensor(2034)\n","tensor(2383)\n","tensor(893)\n","tensor(2376)\n","tensor(1534)\n","tensor(1015)\n","tensor(2467)\n","tensor(1487)\n","tensor(1958)\n","tensor(1885)\n","tensor(854)\n","tensor(249)\n","tensor(1538)\n","tensor(282)\n","tensor(2527)\n","tensor(67)\n","tensor(2239)\n","tensor(2237)\n","tensor(1969)\n","tensor(706)\n","tensor(627)\n","tensor(1968)\n","tensor(1966)\n","tensor(1906)\n","tensor(1365)\n","tensor(463)\n","tensor(1971)\n","tensor(759)\n","tensor(1970)\n","tensor(1240)\n","tensor(2357)\n","tensor(1965)\n","tensor(1532)\n","tensor(1522)\n","tensor(1349)\n","tensor(2282)\n","tensor(2217)\n","tensor(161)\n","tensor(118)\n","tensor(1372)\n","tensor(2597)\n","tensor(2413)\n","tensor(2334)\n","tensor(1740)\n","tensor(1427)\n","tensor(2596)\n","tensor(557)\n","tensor(1070)\n","tensor(1308)\n","tensor(1229)\n","tensor(1103)\n","tensor(53)\n","tensor(1759)\n","tensor(873)\n","tensor(831)\n","tensor(626)\n","tensor(1164)\n","tensor(481)\n","tensor(2448)\n","tensor(2307)\n","tensor(1830)\n","tensor(1216)\n","tensor(936)\n","tensor(170)\n","tensor(2443)\n","tensor(2447)\n","tensor(2446)\n","tensor(1041)\n","tensor(883)\n","tensor(534)\n","tensor(2445)\n","tensor(1765)\n","tensor(1758)\n","tensor(1742)\n","tensor(744)\n","tensor(671)\n","tensor(1583)\n","tensor(1142)\n","tensor(2019)\n","tensor(252)\n","tensor(630)\n","tensor(454)\n","tensor(2442)\n","tensor(494)\n","tensor(417)\n","tensor(228)\n","tensor(719)\n","tensor(1780)\n","tensor(887)\n","tensor(1770)\n","tensor(1166)\n","tensor(926)\n","tensor(1866)\n","tensor(1809)\n","tensor(1819)\n","tensor(1576)\n","tensor(1821)\n","tensor(1121)\n","tensor(1581)\n","tensor(1813)\n","tensor(482)\n","tensor(835)\n","tensor(1808)\n","tensor(1323)\n","tensor(1867)\n","tensor(1858)\n","tensor(1853)\n","tensor(467)\n","tensor(1023)\n","tensor(216)\n","tensor(1212)\n","tensor(1154)\n","tensor(2423)\n","tensor(645)\n","tensor(1459)\n","tensor(1311)\n","tensor(1245)\n","tensor(655)\n","tensor(1773)\n","tensor(1798)\n","tensor(1772)\n","tensor(1705)\n","tensor(859)\n","tensor(1426)\n","tensor(1589)\n","tensor(1486)\n","tensor(1724)\n","tensor(768)\n","tensor(1716)\n","tensor(1499)\n","tensor(337)\n","tensor(2359)\n","tensor(2554)\n","tensor(1833)\n","tensor(1801)\n","tensor(1433)\n","tensor(915)\n","tensor(2584)\n","tensor(2168)\n","tensor(800)\n","tensor(2585)\n","tensor(1706)\n","tensor(1834)\n","tensor(1280)\n","tensor(1750)\n","tensor(284)\n","tensor(2226)\n","tensor(1555)\n","tensor(1953)\n","tensor(1276)\n","tensor(1122)\n","tensor(193)\n","tensor(897)\n","tensor(438)\n","tensor(1218)\n","tensor(1344)\n","tensor(325)\n","tensor(2156)\n","tensor(2023)\n","tensor(1984)\n","tensor(1667)\n","tensor(885)\n","tensor(2022)\n","tensor(2309)\n","tensor(1907)\n","tensor(1905)\n","tensor(1671)\n","tensor(1626)\n","tensor(1614)\n","tensor(164)\n","tensor(37)\n","tensor(530)\n","tensor(441)\n","tensor(2591)\n","tensor(1089)\n","tensor(1788)\n","tensor(1699)\n","tensor(1624)\n","tensor(773)\n","tensor(2254)\n","tensor(1506)\n","tensor(127)\n","tensor(18)\n","tensor(1651)\n","tensor(603)\n","tensor(1248)\n","tensor(215)\n","tensor(1787)\n","tensor(341)\n","tensor(2096)\n","tensor(1346)\n","tensor(519)\n","tensor(308)\n","tensor(303)\n","tensor(1258)\n","tensor(1769)\n","tensor(2365)\n","tensor(493)\n","tensor(1382)\n","tensor(1091)\n","tensor(982)\n","tensor(595)\n","tensor(1205)\n","tensor(223)\n","tensor(561)\n","tensor(2281)\n","tensor(1980)\n","tensor(1203)\n","tensor(1978)\n","tensor(1882)\n","tensor(1314)\n","tensor(478)\n","tensor(1848)\n","tensor(497)\n","tensor(2295)\n","tensor(2153)\n","tensor(2293)\n","tensor(1143)\n","tensor(1131)\n","tensor(1152)\n","tensor(539)\n","tensor(1926)\n","tensor(1924)\n","tensor(1921)\n","tensor(1922)\n","tensor(1602)\n","tensor(572)\n","tensor(149)\n","tensor(30)\n","tensor(1925)\n","tensor(624)\n","tensor(1025)\n","tensor(1047)\n","tensor(901)\n","tensor(485)\n","tensor(2333)\n","tensor(1125)\n","tensor(94)\n","tensor(2347)\n","tensor(2199)\n","tensor(1580)\n","tensor(456)\n","tensor(1596)\n","tensor(524)\n","tensor(1687)\n","tensor(689)\n","tensor(687)\n","tensor(2075)\n","tensor(486)\n","tensor(14)\n","tensor(2076)\n","tensor(1020)\n","tensor(2541)\n","tensor(994)\n","tensor(78)\n","tensor(862)\n","tensor(2291)\n","tensor(794)\n","tensor(2035)\n","tensor(678)\n","tensor(1329)\n","tensor(2493)\n","tensor(1952)\n","tensor(1951)\n","tensor(788)\n","tensor(429)\n","tensor(2398)\n","tensor(1385)\n","tensor(148)\n","tensor(2707)\n","tensor(316)\n","tensor(1870)\n","tensor(480)\n","tensor(1299)\n","tensor(637)\n","tensor(1100)\n","tensor(547)\n","tensor(165)\n","tensor(1572)\n","tensor(2078)\n","tensor(1735)\n","tensor(960)\n","tensor(1283)\n","tensor(2079)\n","tensor(2046)\n","tensor(126)\n","tensor(2393)\n","tensor(1461)\n","tensor(1050)\n","tensor(2250)\n","tensor(2248)\n","tensor(2050)\n","tensor(1402)\n","tensor(2638)\n","tensor(2232)\n","tensor(2233)\n","tensor(858)\n","tensor(2180)\n","tensor(95)\n","tensor(2072)\n","tensor(1285)\n","tensor(1515)\n","tensor(525)\n","tensor(2054)\n","tensor(921)\n","tensor(1918)\n","tensor(69)\n","tensor(830)\n","tensor(1362)\n","tensor(506)\n","tensor(1675)\n","tensor(778)\n","tensor(728)\n","tensor(711)\n","tensor(992)\n","tensor(496)\n","tensor(2388)\n","tensor(2135)\n","tensor(955)\n","tensor(1738)\n","tensor(686)\n","tensor(1284)\n","tensor(902)\n","tensor(2621)\n","tensor(119)\n","tensor(1963)\n","tensor(1962)\n","tensor(1961)\n","tensor(1168)\n","tensor(1360)\n","tensor(1312)\n","tensor(647)\n","tensor(1880)\n","tensor(1898)\n","tensor(2066)\n","tensor(196)\n","tensor(2378)\n","tensor(2348)\n","tensor(574)\n","tensor(503)\n","tensor(2587)\n","tensor(2387)\n","tensor(2336)\n","tensor(1510)\n","tensor(2216)\n","tensor(1201)\n","tensor(421)\n","tensor(2212)\n","tensor(2453)\n","tensor(2424)\n","tensor(1204)\n","tensor(979)\n","tensor(2384)\n","tensor(2136)\n","tensor(1894)\n","tensor(1410)\n","tensor(1896)\n","tensor(142)\n","tensor(2056)\n","tensor(2055)\n","tensor(1635)\n","tensor(597)\n","tensor(1117)\n","tensor(984)\n","tensor(1909)\n","tensor(1376)\n","tensor(1274)\n","tensor(1220)\n","tensor(1069)\n","tensor(1658)\n","tensor(621)\n","tensor(1756)\n","tensor(364)\n","tensor(358)\n","tensor(2503)\n","tensor(1710)\n","tensor(464)\n","tensor(466)\n","tensor(1333)\n","tensor(2163)\n","tensor(738)\n","tensor(2103)\n","tensor(1677)\n","tensor(995)\n","tensor(1338)\n","tensor(2102)\n","tensor(2344)\n","tensor(697)\n","tensor(1609)\n","tensor(695)\n","tensor(191)\n","tensor(2081)\n","tensor(542)\n","tensor(910)\n","tensor(660)\n","tensor(1547)\n","tensor(1546)\n","tensor(683)\n","tensor(1045)\n","tensor(935)\n","tensor(344)\n","tensor(807)\n","tensor(2275)\n","tensor(2274)\n","tensor(2273)\n","tensor(648)\n","tensor(2271)\n","tensor(265)\n","tensor(1707)\n","tensor(241)\n","tensor(301)\n","tensor(853)\n","tensor(1720)\n","tensor(439)\n","tensor(2220)\n","tensor(1737)\n","tensor(1734)\n","tensor(1714)\n","tensor(1145)\n","tensor(2265)\n","tensor(1920)\n","tensor(743)\n","tensor(1490)\n","tensor(289)\n","tensor(2368)\n","tensor(1908)\n","tensor(1149)\n","tensor(2004)\n","tensor(1697)\n","tensor(203)\n","tensor(1712)\n","tensor(326)\n","tensor(1560)\n","tensor(1822)\n","tensor(1095)\n","tensor(568)\n","tensor(1816)\n","tensor(158)\n","tensor(343)\n","tensor(1857)\n","tensor(1869)\n","tensor(1852)\n","tensor(1535)\n","tensor(749)\n","tensor(305)\n","tensor(1008)\n","tensor(1923)\n","tensor(2198)\n","tensor(2304)\n","tensor(383)\n","tensor(1817)\n","tensor(845)\n","tensor(2324)\n","tensor(2131)\n","tensor(2133)\n","tensor(1919)\n","tensor(779)\n","tensor(1883)\n","tensor(590)\n","tensor(1128)\n","tensor(1040)\n","tensor(1696)\n","tensor(1268)\n","tensor(634)\n","tensor(2450)\n","tensor(331)\n","tensor(2426)\n","tensor(2099)\n","tensor(1526)\n","tensor(1341)\n","tensor(2327)\n","tensor(2300)\n","tensor(1061)\n","tensor(943)\n","tensor(589)\n","tensor(121)\n","tensor(556)\n","tensor(2326)\n","tensor(1792)\n","tensor(1197)\n","tensor(741)\n","tensor(1681)\n","tensor(1789)\n","tensor(2369)\n","tensor(2371)\n","tensor(398)\n","tensor(1865)\n","tensor(395)\n","tensor(1845)\n","tensor(2420)\n","tensor(852)\n","tensor(1332)\n","tensor(1269)\n","tensor(1974)\n","tensor(1630)\n","tensor(1972)\n","tensor(725)\n","tensor(604)\n","tensor(1985)\n","tensor(718)\n","tensor(878)\n","tensor(668)\n","tensor(1494)\n","tensor(2116)\n","tensor(1259)\n","tensor(1840)\n","tensor(1592)\n","tensor(1464)\n","tensor(1895)\n","tensor(682)\n","tensor(2154)\n","tensor(1975)\n","tensor(1370)\n","tensor(1374)\n","tensor(274)\n","tensor(666)\n","tensor(32)\n","tensor(1999)\n","tensor(1085)\n","tensor(2395)\n","tensor(2228)\n","tensor(1396)\n","tensor(1027)\n","tensor(2404)\n","tensor(553)\n","tensor(160)\n","tensor(1247)\n","tensor(1542)\n","tensor(25)\n","tensor(2142)\n","tensor(2430)\n","tensor(570)\n","tensor(297)\n","tensor(1007)\n","tensor(533)\n","tensor(425)\n","tensor(1398)\n","tensor(244)\n","tensor(1528)\n","tensor(1361)\n","tensor(513)\n","tensor(881)\n","tensor(1733)\n","tensor(2459)\n","tensor(985)\n","tensor(2358)\n","tensor(328)\n","tensor(2161)\n","tensor(1328)\n","tensor(898)\n","tensor(23)\n","tensor(2083)\n","tensor(1779)\n","tensor(843)\n","tensor(1180)\n","tensor(573)\n","tensor(2084)\n","tensor(1115)\n","tensor(1097)\n","tensor(857)\n","tensor(2009)\n","tensor(1127)\n","tensor(2509)\n","tensor(1295)\n","tensor(2077)\n","tensor(2412)\n","tensor(1347)\n","tensor(460)\n","tensor(2379)\n","tensor(1055)\n","tensor(2335)\n","tensor(2236)\n","tensor(2361)\n","tensor(2360)\n","tensor(531)\n","tensor(27)\n","tensor(2696)\n","tensor(185)\n","tensor(576)\n","tensor(397)\n","tensor(2401)\n","tensor(1087)\n","tensor(375)\n","tensor(1350)\n","tensor(1401)\n","tensor(2465)\n","tensor(2290)\n","tensor(515)\n","tensor(1795)\n","tensor(1207)\n","tensor(275)\n","tensor(2645)\n","tensor(808)\n","tensor(317)\n","tensor(2350)\n","tensor(2260)\n","tensor(1529)\n","tensor(529)\n","tensor(2463)\n","tensor(2294)\n","tensor(1348)\n","tensor(2038)\n","tensor(1929)\n","tensor(1889)\n","tensor(1002)\n","tensor(2382)\n","tensor(814)\n","tensor(299)\n","tensor(783)\n","tensor(638)\n","tensor(1428)\n","tensor(813)\n","tensor(1519)\n","tensor(1429)\n","tensor(1082)\n","tensor(2262)\n","tensor(1481)\n","tensor(2468)\n","tensor(1501)\n","tensor(739)\n","tensor(1881)\n","tensor(1237)\n","tensor(735)\n","tensor(2644)\n","tensor(643)\n","tensor(1500)\n","tensor(2628)\n","tensor(1569)\n","tensor(946)\n","tensor(443)\n","tensor(2059)\n","tensor(1964)\n","tensor(2519)\n","tensor(271)\n","tensor(789)\n","tensor(2356)\n","tensor(2355)\n","tensor(1846)\n","tensor(2648)\n","tensor(2680)\n","tensor(2684)\n","tensor(1320)\n","tensor(1351)\n","tensor(231)\n","tensor(96)\n","tensor(2112)\n","tensor(2030)\n","tensor(1343)\n","tensor(2415)\n","tensor(2414)\n","tensor(174)\n","tensor(2391)\n","tensor(657)\n","tensor(2390)\n","tensor(1804)\n","tensor(860)\n","tensor(1470)\n","tensor(1167)\n","tensor(1254)\n","tensor(1384)\n","tensor(1749)\n","tensor(2439)\n","tensor(867)\n","tensor(1480)\n","tensor(1760)\n","tensor(1123)\n","tensor(1739)\n","tensor(569)\n","tensor(2313)\n","tensor(1708)\n","tensor(1479)\n","tensor(261)\n","tensor(1011)\n","tensor(136)\n","tensor(2558)\n","tensor(2556)\n","tensor(372)\n","tensor(823)\n","tensor(762)\n","tensor(889)\n","tensor(526)\n","tensor(2641)\n","tensor(2530)\n","tensor(2515)\n","tensor(246)\n","tensor(1037)\n","tensor(1466)\n","tensor(933)\n","tensor(629)\n","tensor(227)\n","tensor(1562)\n","tensor(1755)\n","tensor(1492)\n","tensor(403)\n","tensor(2025)\n","tensor(451)\n","tensor(1995)\n","tensor(1120)\n","tensor(2485)\n","tensor(1973)\n","tensor(2190)\n","tensor(2189)\n","tensor(2188)\n","tensor(1917)\n","tensor(2494)\n","tensor(848)\n","tensor(322)\n","tensor(302)\n","tensor(1806)\n","tensor(962)\n","tensor(1948)\n","tensor(1331)\n","tensor(716)\n","tensor(310)\n","tensor(1424)\n","tensor(2308)\n","tensor(1812)\n","tensor(1807)\n","tensor(1654)\n","tensor(1134)\n","tensor(579)\n","tensor(2117)\n","tensor(1172)\n","tensor(2568)\n","tensor(357)\n","tensor(518)\n","tensor(304)\n","tensor(279)\n","tensor(1901)\n","tensor(1267)\n","tensor(891)\n","tensor(2218)\n","tensor(1313)\n","tensor(468)\n","tensor(1144)\n","tensor(329)\n","tensor(1030)\n","tensor(101)\n","tensor(650)\n","tensor(2689)\n","tensor(2451)\n","tensor(2203)\n","tensor(931)\n","tensor(1033)\n","tensor(820)\n","tensor(2612)\n","tensor(1766)\n","tensor(146)\n","tensor(1927)\n","tensor(2315)\n","tensor(1928)\n","tensor(724)\n","tensor(324)\n","tensor(1512)\n","tensor(1266)\n","tensor(1178)\n","tensor(2386)\n","tensor(756)\n","tensor(1399)\n","tensor(461)\n","tensor(717)\n","tensor(566)\n","tensor(2427)\n","tensor(1190)\n","tensor(2184)\n","tensor(1455)\n","tensor(70)\n","tensor(234)\n","tensor(1088)\n","tensor(1065)\n","tensor(1786)\n","tensor(2145)\n","tensor(1873)\n","tensor(1389)\n","tensor(613)\n","tensor(1856)\n","tensor(1009)\n","tensor(937)\n","tensor(416)\n","tensor(1574)\n","tensor(1253)\n","tensor(1709)\n","tensor(1565)\n","tensor(998)\n","tensor(616)\n","tensor(1244)\n","tensor(2283)\n","tensor(1502)\n","tensor(281)\n","tensor(1458)\n","tensor(319)\n","tensor(1215)\n","tensor(424)\n","tensor(382)\n","tensor(516)\n","tensor(890)\n","tensor(1110)\n","tensor(86)\n","tensor(1618)\n","tensor(2185)\n","tensor(838)\n","tensor(836)\n","tensor(2416)\n","tensor(2051)\n","tensor(942)\n","tensor(827)\n","tensor(315)\n","tensor(2074)\n","tensor(2073)\n","tensor(2343)\n","tensor(2620)\n","tensor(1232)\n","tensor(373)\n","tensor(1588)\n","tensor(2186)\n","tensor(1279)\n","tensor(731)\n","tensor(2219)\n","tensor(839)\n","tensor(62)\n","tensor(2332)\n","tensor(2263)\n","tensor(934)\n","tensor(1692)\n","tensor(2201)\n","tensor(2200)\n","tensor(1892)\n","tensor(552)\n","tensor(1003)\n","tensor(28)\n","tensor(1317)\n","tensor(1292)\n","tensor(2091)\n","tensor(435)\n","tensor(180)\n","tensor(1591)\n","tensor(1418)\n","tensor(38)\n","tensor(715)\n","tensor(2268)\n","tensor(1265)\n","tensor(1590)\n","tensor(1160)\n","tensor(863)\n","tensor(1324)\n","tensor(377)\n","tensor(855)\n","tensor(157)\n","tensor(2204)\n","tensor(622)\n","tensor(2033)\n","tensor(523)\n","tensor(705)\n","tensor(2518)\n","tensor(602)\n","tensor(381)\n","tensor(378)\n","tensor(2706)\n","tensor(1473)\n","tensor(766)\n","tensor(1073)\n","tensor(143)\n","tensor(2372)\n","tensor(1641)\n","tensor(562)\n","tensor(969)\n","tensor(1282)\n","tensor(2650)\n","tensor(784)\n","tensor(2048)\n","tensor(1551)\n","tensor(368)\n","tensor(2564)\n","tensor(194)\n","tensor(1230)\n","tensor(1342)\n","tensor(1260)\n","tensor(348)\n","tensor(283)\n","tensor(996)\n","tensor(1545)\n","tensor(1135)\n","tensor(1255)\n","tensor(1482)\n","tensor(1163)\n","tensor(636)\n","tensor(2502)\n","tensor(2183)\n","tensor(257)\n","tensor(81)\n","tensor(1395)\n","tensor(2234)\n","tensor(2181)\n","tensor(2057)\n","tensor(2134)\n","tensor(2100)\n","tensor(1352)\n","tensor(1676)\n","tensor(1074)\n","tensor(2553)\n","tensor(1296)\n","tensor(2113)\n","tensor(2111)\n","tensor(1377)\n","tensor(945)\n","tensor(190)\n","tensor(757)\n","tensor(1537)\n","tensor(646)\n","tensor(801)\n","tensor(703)\n","tensor(1955)\n","tensor(1960)\n","tensor(1904)\n","tensor(1902)\n","tensor(753)\n","tensor(755)\n","tensor(2065)\n","tensor(1477)\n","tensor(1098)\n","tensor(1018)\n","tensor(1406)\n","tensor(2524)\n","tensor(1202)\n","tensor(1162)\n","tensor(2533)\n","tensor(708)\n","tensor(195)\n","tensor(1838)\n","tensor(1330)\n","tensor(751)\n","tensor(2207)\n","tensor(1957)\n","tensor(1503)\n","tensor(1049)\n","tensor(1897)\n","tensor(793)\n","tensor(709)\n","tensor(2049)\n","tensor(2576)\n","tensor(2311)\n","tensor(1680)\n","tensor(734)\n","tensor(1158)\n","tensor(763)\n","tensor(1104)\n","tensor(2499)\n","tensor(434)\n","tensor(80)\n","tensor(1068)\n","tensor(1171)\n","tensor(732)\n","tensor(819)\n","tensor(2546)\n","tensor(1305)\n","tensor(2195)\n","tensor(1303)\n","tensor(405)\n","tensor(2385)\n","tensor(2475)\n","tensor(1791)\n","tensor(407)\n","tensor(1336)\n","tensor(1682)\n","tensor(1012)\n","tensor(1571)\n","tensor(2323)\n","tensor(389)\n","tensor(1956)\n","tensor(1226)\n","tensor(1954)\n","tensor(1642)\n","tensor(2525)\n","tensor(2221)\n","tensor(1148)\n","tensor(2616)\n","tensor(1888)\n","tensor(1209)\n","tensor(64)\n","tensor(1727)\n","tensor(2422)\n"]}],"source":["for i in n_id:\n","    print(i)"]},{"cell_type":"markdown","metadata":{},"source":["### SAGEConv"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class SAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n","        super().__init__()\n","\n","        self.num_layers = num_layers\n","\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(SAGEConv(in_channels, hidden_channels))\n","        for _ in range(num_layers - 2):\n","            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n","        self.convs.append(SAGEConv(hidden_channels, out_channels))\n","\n","    def reset_parameters(self):\n","        for conv in self.convs:\n","            conv.reset_parameters()\n","\n","    def forward(self, x, adjs):\n","        # `train_loader` computes the k-hop neighborhood of a batch of nodes,\n","        # and returns, for each layer, a bipartite graph object, holding the\n","        # bipartite edges `edge_index`, the index `e_id` of the original edges,\n","        # and the size/shape `size` of the bipartite graph.\n","        # Target nodes are also included in the source nodes so that one can\n","        # easily apply skip-connections or add self-loops.\n","        for i, (edge_index, _, size) in enumerate(adjs):\n","            # 对每一层的bipartite图都有x_target = x[:size[1]]\n","            x_target = x[:size[1]]  # Target nodes are always placed first.目标节点放在最前面，一共有size[1]个目标节点\n","            # 实现了对一层bipartite图的卷积。可以把卷积就理解为聚合操作，这里就是逐层聚合，从第L层到第1层\n","            x = self.convs[i]((x, x_target), edge_index)\n","            if i != self.num_layers - 1:  # 不是最后一层就执行下面的操作\n","                x = F.relu(x)\n","                x = F.dropout(x, p=0.5, training=self.training)\n","        return x.log_softmax(dim=-1)\n","\n","    def inference(self, x_all):\n","        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n","        pbar.set_description('Evaluating')\n","\n","        # Compute representations of nodes layer by layer, using *all*\n","        # available edges. This leads to faster computation in contrast to\n","        # immediately computing the final representations of each batch.\n","        total_edges = 0\n","        for i in range(self.num_layers): # 一共有l层\n","            xs = []\n","            # 一个batchsize中的目标节点采样L=1层涉及到的所有节点\n","            for batch_size, n_id, adj in subgraph_loader:\n","                edge_index, _, size = adj.to(device)\n","                total_edges += edge_index.size(1)\n","                x = x_all[n_id].to(device)\n","                x_target = x[:size[1]]\n","                x = self.convs[i]((x, x_target), edge_index)\n","                if i != self.num_layers - 1:\n","                    x = F.relu(x)\n","                xs.append(x.cpu())\n","\n","                pbar.update(batch_size)\n","\n","            x_all = torch.cat(xs, dim=0)\n","\n","        pbar.close()\n","\n","        return x_all"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = SAGE(dataset.num_features, 64, dataset.num_classes, num_layers=3)\n","model = model.to(device)\n","\n","x = data.x.to(device)\n","y = data.y.squeeze().to(device)\n","criterion = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def train(epoch):\n","    model.train()\n","\n","    pbar = tqdm(total=train_idx.size(0))\n","    pbar.set_description(f'Epoch {epoch:02d}')\n","\n","    total_loss = total_correct = 0\n","\n","    for batch_size, n_id, adjs in train_loader:\n","        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n","        adjs = [adj.to(device) for adj in adjs]\n","\n","        optimizer.zero_grad()\n","        out = model(x[n_id], adjs)  # x[n_id]这个batchsize中的目标节点采样L层涉及到的所有节点\n","        loss = criterion(out, y[n_id[:batch_size]])\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += float(loss)\n","        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n","        pbar.update(batch_size)\n","\n","    pbar.close()\n","\n","    loss = total_loss / len(train_loader)\n","    approx_acc = total_correct / train_idx.size(0)\n","\n","    return loss, approx_acc"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["@torch.no_grad()\n","def test():\n","    model.eval()\n","\n","    out = model.inference(x)\n","\n","    y_true = y.cpu().unsqueeze(-1)\n","    y_pred = out.argmax(dim=-1, keepdim=True)\n","    correct = (y_pred == y_true).sum().item()\n","    test_acc = correct/data.num_nodes\n","    return test_acc"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Run 01:\n","\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 01: 100%|██████████| 140/140 [00:00<00:00, 421.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 01, Loss: 1.9465, Approx. Train: 0.1214\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 02: 100%|██████████| 140/140 [00:00<00:00, 1137.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 02, Loss: 1.8948, Approx. Train: 0.3071\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 03: 100%|██████████| 140/140 [00:00<00:00, 1149.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 03, Loss: 1.8147, Approx. Train: 0.5571\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 04: 100%|██████████| 140/140 [00:00<00:00, 1197.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 04, Loss: 1.7139, Approx. Train: 0.6500\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 05: 100%|██████████| 140/140 [00:00<00:00, 1133.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 05, Loss: 1.5318, Approx. Train: 0.7714\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 06: 100%|██████████| 140/140 [00:00<00:00, 1198.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 06, Loss: 1.2959, Approx. Train: 0.7714\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 07: 100%|██████████| 140/140 [00:00<00:00, 1200.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 07, Loss: 1.0412, Approx. Train: 0.8786\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 08: 100%|██████████| 140/140 [00:00<00:00, 1206.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 08, Loss: 0.7816, Approx. Train: 0.9143\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 09: 100%|██████████| 140/140 [00:00<00:00, 1197.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 09, Loss: 0.5457, Approx. Train: 0.9286\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 8124/8124 [00:00<00:00, 19723.86it/s]"]},{"name":"stdout","output_type":"stream","text":["Test: 0.8154\n","============================\n","Final Test: 0.8154 ± nan\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["test_accs = []\n","for run in range(1, 2):#11\n","    print('')\n","    print(f'Run {run:02d}:')\n","    print('')\n","\n","    model.reset_parameters()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n","\n","    best_val_acc = final_test_acc = 0\n","    for epoch in range(1, 10):#51\n","        loss, acc = train(epoch)\n","        print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n","\n","\n","    test_acc = test()\n","    print(f'Test: {test_acc:.4f}')\n","    test_accs.append(test_acc)\n","\n","test_acc = torch.tensor(test_accs)\n","print('============================')\n","print(f'Final Test: {test_acc.mean():.4f} ± {test_acc.std():.4f}')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
