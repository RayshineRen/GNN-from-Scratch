{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cora()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Planetoid(\n",
    "    root=\"/home/ray/code/python/python_data_course/机器学习与深度学习导论/data\", name=\"Cora\"\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSageNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphSageNet, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = SAGEConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphSageNet(\n",
       "  (conv1): SAGEConv(1433, 16, aggr=mean)\n",
       "  (conv2): SAGEConv(16, 7, aggr=mean)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GraphSageNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train_loss: 1.9451 train_acc: 0.1286\n",
      "Epoch 001 train_loss: 1.9237 train_acc: 0.3714\n",
      "Epoch 002 train_loss: 1.8896 train_acc: 0.5500\n",
      "Epoch 003 train_loss: 1.8303 train_acc: 0.6071\n",
      "Epoch 004 train_loss: 1.7631 train_acc: 0.6714\n",
      "Epoch 005 train_loss: 1.6962 train_acc: 0.7357\n",
      "Epoch 006 train_loss: 1.6212 train_acc: 0.7429\n",
      "Epoch 007 train_loss: 1.5604 train_acc: 0.8429\n",
      "Epoch 008 train_loss: 1.5042 train_acc: 0.9143\n",
      "Epoch 009 train_loss: 1.4347 train_acc: 0.9214\n",
      "Epoch 010 train_loss: 1.3956 train_acc: 0.9500\n",
      "Epoch 011 train_loss: 1.3594 train_acc: 0.9500\n",
      "Epoch 012 train_loss: 1.3364 train_acc: 0.9714\n",
      "Epoch 013 train_loss: 1.3175 train_acc: 0.9429\n",
      "Epoch 014 train_loss: 1.2701 train_acc: 0.9714\n",
      "Epoch 015 train_loss: 1.2644 train_acc: 0.9714\n",
      "Epoch 016 train_loss: 1.2396 train_acc: 0.9786\n",
      "Epoch 017 train_loss: 1.2266 train_acc: 0.9857\n",
      "Epoch 018 train_loss: 1.2243 train_acc: 1.0000\n",
      "Epoch 019 train_loss: 1.2020 train_acc: 0.9929\n",
      "Epoch 020 train_loss: 1.2011 train_acc: 1.0000\n",
      "Epoch 021 train_loss: 1.2031 train_acc: 0.9857\n",
      "Epoch 022 train_loss: 1.2028 train_acc: 1.0000\n",
      "Epoch 023 train_loss: 1.1993 train_acc: 0.9929\n",
      "Epoch 024 train_loss: 1.1850 train_acc: 1.0000\n",
      "Epoch 025 train_loss: 1.2010 train_acc: 0.9786\n",
      "Epoch 026 train_loss: 1.1965 train_acc: 1.0000\n",
      "Epoch 027 train_loss: 1.1888 train_acc: 0.9929\n",
      "Epoch 028 train_loss: 1.1858 train_acc: 1.0000\n",
      "Epoch 029 train_loss: 1.2048 train_acc: 0.9857\n",
      "Epoch 030 train_loss: 1.1887 train_acc: 1.0000\n",
      "Epoch 031 train_loss: 1.1797 train_acc: 1.0000\n",
      "Epoch 032 train_loss: 1.1831 train_acc: 1.0000\n",
      "Epoch 033 train_loss: 1.1853 train_acc: 1.0000\n",
      "Epoch 034 train_loss: 1.1851 train_acc: 0.9857\n",
      "Epoch 035 train_loss: 1.1819 train_acc: 1.0000\n",
      "Epoch 036 train_loss: 1.1942 train_acc: 0.9929\n",
      "Epoch 037 train_loss: 1.1819 train_acc: 1.0000\n",
      "Epoch 038 train_loss: 1.1847 train_acc: 0.9929\n",
      "Epoch 039 train_loss: 1.1812 train_acc: 1.0000\n",
      "Epoch 040 train_loss: 1.1749 train_acc: 1.0000\n",
      "Epoch 041 train_loss: 1.1911 train_acc: 0.9929\n",
      "Epoch 042 train_loss: 1.1881 train_acc: 0.9929\n",
      "Epoch 043 train_loss: 1.1840 train_acc: 0.9929\n",
      "Epoch 044 train_loss: 1.1904 train_acc: 0.9929\n",
      "Epoch 045 train_loss: 1.1838 train_acc: 0.9929\n",
      "Epoch 046 train_loss: 1.1854 train_acc: 0.9929\n",
      "Epoch 047 train_loss: 1.1836 train_acc: 0.9929\n",
      "Epoch 048 train_loss: 1.1852 train_acc: 0.9929\n",
      "Epoch 049 train_loss: 1.1768 train_acc: 1.0000\n",
      "Epoch 050 train_loss: 1.1841 train_acc: 1.0000\n",
      "Epoch 051 train_loss: 1.1761 train_acc: 1.0000\n",
      "Epoch 052 train_loss: 1.1828 train_acc: 1.0000\n",
      "Epoch 053 train_loss: 1.1877 train_acc: 1.0000\n",
      "Epoch 054 train_loss: 1.1793 train_acc: 1.0000\n",
      "Epoch 055 train_loss: 1.1806 train_acc: 1.0000\n",
      "Epoch 056 train_loss: 1.1901 train_acc: 0.9929\n",
      "Epoch 057 train_loss: 1.1784 train_acc: 1.0000\n",
      "Epoch 058 train_loss: 1.1877 train_acc: 1.0000\n",
      "Epoch 059 train_loss: 1.1870 train_acc: 0.9929\n",
      "Epoch 060 train_loss: 1.1856 train_acc: 0.9929\n",
      "Epoch 061 train_loss: 1.1919 train_acc: 1.0000\n",
      "Epoch 062 train_loss: 1.1843 train_acc: 1.0000\n",
      "Epoch 063 train_loss: 1.1789 train_acc: 1.0000\n",
      "Epoch 064 train_loss: 1.1863 train_acc: 0.9929\n",
      "Epoch 065 train_loss: 1.1848 train_acc: 0.9857\n",
      "Epoch 066 train_loss: 1.1805 train_acc: 1.0000\n",
      "Epoch 067 train_loss: 1.1828 train_acc: 1.0000\n",
      "Epoch 068 train_loss: 1.1867 train_acc: 0.9929\n",
      "Epoch 069 train_loss: 1.1794 train_acc: 1.0000\n",
      "Epoch 070 train_loss: 1.1787 train_acc: 1.0000\n",
      "Epoch 071 train_loss: 1.1779 train_acc: 1.0000\n",
      "Epoch 072 train_loss: 1.1863 train_acc: 1.0000\n",
      "Epoch 073 train_loss: 1.1819 train_acc: 1.0000\n",
      "Epoch 074 train_loss: 1.1811 train_acc: 1.0000\n",
      "Epoch 075 train_loss: 1.1813 train_acc: 1.0000\n",
      "Epoch 076 train_loss: 1.1839 train_acc: 0.9929\n",
      "Epoch 077 train_loss: 1.1835 train_acc: 1.0000\n",
      "Epoch 078 train_loss: 1.1801 train_acc: 1.0000\n",
      "Epoch 079 train_loss: 1.1797 train_acc: 1.0000\n",
      "Epoch 080 train_loss: 1.1943 train_acc: 0.9929\n",
      "Epoch 081 train_loss: 1.1849 train_acc: 1.0000\n",
      "Epoch 082 train_loss: 1.1763 train_acc: 1.0000\n",
      "Epoch 083 train_loss: 1.1740 train_acc: 1.0000\n",
      "Epoch 084 train_loss: 1.1920 train_acc: 0.9929\n",
      "Epoch 085 train_loss: 1.1876 train_acc: 0.9929\n",
      "Epoch 086 train_loss: 1.1748 train_acc: 1.0000\n",
      "Epoch 087 train_loss: 1.1851 train_acc: 1.0000\n",
      "Epoch 088 train_loss: 1.1806 train_acc: 1.0000\n",
      "Epoch 089 train_loss: 1.1840 train_acc: 0.9929\n",
      "Epoch 090 train_loss: 1.1811 train_acc: 1.0000\n",
      "Epoch 091 train_loss: 1.1829 train_acc: 0.9929\n",
      "Epoch 092 train_loss: 1.1816 train_acc: 1.0000\n",
      "Epoch 093 train_loss: 1.1932 train_acc: 0.9857\n",
      "Epoch 094 train_loss: 1.1920 train_acc: 0.9857\n",
      "Epoch 095 train_loss: 1.1754 train_acc: 1.0000\n",
      "Epoch 096 train_loss: 1.1831 train_acc: 1.0000\n",
      "Epoch 097 train_loss: 1.1792 train_acc: 1.0000\n",
      "Epoch 098 train_loss: 1.1826 train_acc: 1.0000\n",
      "Epoch 099 train_loss: 1.1829 train_acc: 1.0000\n",
      "Epoch 100 train_loss: 1.1816 train_acc: 1.0000\n",
      "Epoch 101 train_loss: 1.1751 train_acc: 1.0000\n",
      "Epoch 102 train_loss: 1.1866 train_acc: 1.0000\n",
      "Epoch 103 train_loss: 1.1856 train_acc: 0.9929\n",
      "Epoch 104 train_loss: 1.1864 train_acc: 1.0000\n",
      "Epoch 105 train_loss: 1.1860 train_acc: 0.9857\n",
      "Epoch 106 train_loss: 1.1852 train_acc: 1.0000\n",
      "Epoch 107 train_loss: 1.1838 train_acc: 1.0000\n",
      "Epoch 108 train_loss: 1.1797 train_acc: 0.9929\n",
      "Epoch 109 train_loss: 1.1817 train_acc: 0.9929\n",
      "Epoch 110 train_loss: 1.1812 train_acc: 1.0000\n",
      "Epoch 111 train_loss: 1.1850 train_acc: 1.0000\n",
      "Epoch 112 train_loss: 1.1793 train_acc: 0.9929\n",
      "Epoch 113 train_loss: 1.1749 train_acc: 1.0000\n",
      "Epoch 114 train_loss: 1.1776 train_acc: 1.0000\n",
      "Epoch 115 train_loss: 1.1787 train_acc: 1.0000\n",
      "Epoch 116 train_loss: 1.1757 train_acc: 1.0000\n",
      "Epoch 117 train_loss: 1.1789 train_acc: 1.0000\n",
      "Epoch 118 train_loss: 1.1755 train_acc: 1.0000\n",
      "Epoch 119 train_loss: 1.1776 train_acc: 1.0000\n",
      "Epoch 120 train_loss: 1.1784 train_acc: 1.0000\n",
      "Epoch 121 train_loss: 1.1786 train_acc: 1.0000\n",
      "Epoch 122 train_loss: 1.1772 train_acc: 1.0000\n",
      "Epoch 123 train_loss: 1.1772 train_acc: 1.0000\n",
      "Epoch 124 train_loss: 1.1910 train_acc: 0.9929\n",
      "Epoch 125 train_loss: 1.1848 train_acc: 1.0000\n",
      "Epoch 126 train_loss: 1.1833 train_acc: 0.9929\n",
      "Epoch 127 train_loss: 1.1742 train_acc: 1.0000\n",
      "Epoch 128 train_loss: 1.1721 train_acc: 1.0000\n",
      "Epoch 129 train_loss: 1.1850 train_acc: 0.9857\n",
      "Epoch 130 train_loss: 1.1760 train_acc: 1.0000\n",
      "Epoch 131 train_loss: 1.1851 train_acc: 1.0000\n",
      "Epoch 132 train_loss: 1.1798 train_acc: 1.0000\n",
      "Epoch 133 train_loss: 1.1783 train_acc: 1.0000\n",
      "Epoch 134 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 135 train_loss: 1.1832 train_acc: 0.9929\n",
      "Epoch 136 train_loss: 1.1781 train_acc: 0.9929\n",
      "Epoch 137 train_loss: 1.1794 train_acc: 1.0000\n",
      "Epoch 138 train_loss: 1.1787 train_acc: 1.0000\n",
      "Epoch 139 train_loss: 1.1795 train_acc: 0.9929\n",
      "Epoch 140 train_loss: 1.1778 train_acc: 0.9929\n",
      "Epoch 141 train_loss: 1.1760 train_acc: 1.0000\n",
      "Epoch 142 train_loss: 1.1784 train_acc: 1.0000\n",
      "Epoch 143 train_loss: 1.1816 train_acc: 1.0000\n",
      "Epoch 144 train_loss: 1.1755 train_acc: 1.0000\n",
      "Epoch 145 train_loss: 1.1767 train_acc: 1.0000\n",
      "Epoch 146 train_loss: 1.1826 train_acc: 1.0000\n",
      "Epoch 147 train_loss: 1.1825 train_acc: 0.9929\n",
      "Epoch 148 train_loss: 1.1751 train_acc: 1.0000\n",
      "Epoch 149 train_loss: 1.1794 train_acc: 1.0000\n",
      "Epoch 150 train_loss: 1.1761 train_acc: 1.0000\n",
      "Epoch 151 train_loss: 1.1733 train_acc: 1.0000\n",
      "Epoch 152 train_loss: 1.1760 train_acc: 1.0000\n",
      "Epoch 153 train_loss: 1.1777 train_acc: 1.0000\n",
      "Epoch 154 train_loss: 1.1728 train_acc: 1.0000\n",
      "Epoch 155 train_loss: 1.1758 train_acc: 1.0000\n",
      "Epoch 156 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 157 train_loss: 1.1873 train_acc: 0.9929\n",
      "Epoch 158 train_loss: 1.1759 train_acc: 1.0000\n",
      "Epoch 159 train_loss: 1.1791 train_acc: 1.0000\n",
      "Epoch 160 train_loss: 1.1781 train_acc: 1.0000\n",
      "Epoch 161 train_loss: 1.1808 train_acc: 1.0000\n",
      "Epoch 162 train_loss: 1.1794 train_acc: 1.0000\n",
      "Epoch 163 train_loss: 1.1815 train_acc: 1.0000\n",
      "Epoch 164 train_loss: 1.1767 train_acc: 1.0000\n",
      "Epoch 165 train_loss: 1.1766 train_acc: 1.0000\n",
      "Epoch 166 train_loss: 1.1876 train_acc: 0.9857\n",
      "Epoch 167 train_loss: 1.1810 train_acc: 0.9929\n",
      "Epoch 168 train_loss: 1.1721 train_acc: 1.0000\n",
      "Epoch 169 train_loss: 1.1767 train_acc: 1.0000\n",
      "Epoch 170 train_loss: 1.1804 train_acc: 1.0000\n",
      "Epoch 171 train_loss: 1.1785 train_acc: 0.9929\n",
      "Epoch 172 train_loss: 1.1783 train_acc: 1.0000\n",
      "Epoch 173 train_loss: 1.1785 train_acc: 1.0000\n",
      "Epoch 174 train_loss: 1.1788 train_acc: 1.0000\n",
      "Epoch 175 train_loss: 1.1877 train_acc: 0.9857\n",
      "Epoch 176 train_loss: 1.1762 train_acc: 1.0000\n",
      "Epoch 177 train_loss: 1.1781 train_acc: 0.9929\n",
      "Epoch 178 train_loss: 1.1741 train_acc: 1.0000\n",
      "Epoch 179 train_loss: 1.1807 train_acc: 1.0000\n",
      "Epoch 180 train_loss: 1.1821 train_acc: 0.9929\n",
      "Epoch 181 train_loss: 1.1792 train_acc: 1.0000\n",
      "Epoch 182 train_loss: 1.1797 train_acc: 1.0000\n",
      "Epoch 183 train_loss: 1.1717 train_acc: 1.0000\n",
      "Epoch 184 train_loss: 1.1758 train_acc: 1.0000\n",
      "Epoch 185 train_loss: 1.1785 train_acc: 1.0000\n",
      "Epoch 186 train_loss: 1.1755 train_acc: 1.0000\n",
      "Epoch 187 train_loss: 1.1844 train_acc: 0.9929\n",
      "Epoch 188 train_loss: 1.1887 train_acc: 0.9857\n",
      "Epoch 189 train_loss: 1.1791 train_acc: 1.0000\n",
      "Epoch 190 train_loss: 1.1922 train_acc: 0.9786\n",
      "Epoch 191 train_loss: 1.1809 train_acc: 0.9929\n",
      "Epoch 192 train_loss: 1.1779 train_acc: 1.0000\n",
      "Epoch 193 train_loss: 1.1710 train_acc: 1.0000\n",
      "Epoch 194 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 195 train_loss: 1.1741 train_acc: 0.9929\n",
      "Epoch 196 train_loss: 1.1786 train_acc: 1.0000\n",
      "Epoch 197 train_loss: 1.1803 train_acc: 1.0000\n",
      "Epoch 198 train_loss: 1.1799 train_acc: 1.0000\n",
      "Epoch 199 train_loss: 1.1758 train_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(200):\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, pred = torch.max(out[data.train_mask], dim=1)\n",
    "    correct = (pred == data.y[data.train_mask]).sum().item()\n",
    "    acc = correct/data.train_mask.sum().item()\n",
    "    print('Epoch {:03d} train_loss: {:.4f} train_acc: {:.4f}'.format(epoch, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.3794 test_acc: 0.8120\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = model(data)\n",
    "loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
    "_, pred = torch.max(out[data.test_mask], dim=1)\n",
    "correct = (pred == data.y[data.test_mask]).sum().item()\n",
    "acc = correct/data.test_mask.sum().item()\n",
    "print(\"test_loss: {:.4f} test_acc: {:.4f}\".format(loss.item(), acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "percnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
