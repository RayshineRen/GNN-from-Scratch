{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zhuanlan.zhihu.com/p/344067462\n",
    "# https://zhuanlan.zhihu.com/p/142205899\n",
    "import torch as torch\n",
    "import dgl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# 定义图神经网络GraphSAGE\n",
    "from dgl.nn.pytorch import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_feats,\n",
    "                 n_hidden, # hidden size也可以是一个list\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator):\n",
    "        \n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.layer.append(SAGEConv(in_feats, n_hidden, aggregator))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layer.append(SAGEConv(n_hidden, n_hidden, aggregator))\n",
    "        self.layer.append(SAGEConv(n_hidden, n_classes, aggregator))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, blocks, feas):\n",
    "        h = feas\n",
    "        for i, (layer, block) in enumerate(zip(self.layer, blocks)):\n",
    "            h = layer(block, h)\n",
    "            if i != self.n_layers - 1:\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "    \n",
    "    def inference(self, my_net, val_nid, batch_s, num_worker, device):\n",
    "        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(self.n_layers)\n",
    "        dataloader = dgl.dataloading.DataLoader(\n",
    "                    my_net,\n",
    "                    val_nid,\n",
    "                    sampler,\n",
    "                    batch_size = batch_s,\n",
    "                    shuffle=True,\n",
    "                    drop_last=False,\n",
    "                    num_workers=num_worker\n",
    "                )\n",
    "        \n",
    "        ret = torch.zeros(my_net.num_nodes(), self.n_classes)\n",
    "        \n",
    "        for input_nodes, output_nodes, blocks in dataloader:\n",
    "            h = blocks[0].srcdata['features'].to(device)\n",
    "            for i, (layer, block) in enumerate(zip(self.layer, blocks)):\n",
    "                block = block.int().to(device)\n",
    "                h = layer(block, h)\n",
    "                if i != self.n_layers - 1 :\n",
    "                    h = self.activation(h)\n",
    "                    h = self.dropout(h)\n",
    "            ret[output_nodes] = h.cpu()\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subtensor(nfeat, labels, seeds, input_nodes, device):\n",
    "    \"\"\"\n",
    "    Extracts features and labels for a subset of nodes.\n",
    "    \"\"\"\n",
    "    batch_inputs = nfeat[input_nodes].to(device)\n",
    "    batch_labels = labels[seeds].to(device)\n",
    "    return batch_inputs, batch_labels\n",
    "\n",
    "def evaluate(model, my_net, labels, val_nid, val_mask, batch_s, num_worker, device):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        label_pred = model.inference(my_net, val_nid,  batch_s, num_worker, device)\n",
    "    model.train()\n",
    "    return (torch.argmax(label_pred[val_mask], dim=1) == labels[val_mask]).float().sum() / len(label_pred[val_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写一个分batch训练的过程\n",
    "import itertools\n",
    "\n",
    "def run(data, train_val_data, args, sample_size, learning_rate, device):\n",
    "    in_feats, n_classes, my_net, fea_para = data\n",
    "    hidden_size, n_layers, activation, dropout, aggregator, batch_s, num_worker  = args\n",
    "    \n",
    "#     my_net = my_net.to(device)\n",
    "    \n",
    "    # 设置一下训练集和测试集，val_mask\n",
    "    train_mask, test_mask, val_mask, train_nid, test_nid, val_nid = train_val_data\n",
    "    \n",
    "    # 训练模型的过程\n",
    "    nfeat = my_net.ndata['features']\n",
    "    labels = my_net.ndata['label']\n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler(sample_size)\n",
    "    \n",
    "    dataloader = dgl.dataloading.DataLoader(\n",
    "        my_net,\n",
    "        train_nid,\n",
    "        sampler,\n",
    "        batch_size = batch_s,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=num_worker\n",
    "    )\n",
    "    \n",
    "    model = GraphSAGE(in_feats, hidden_size, n_classes, n_layers, activation, dropout, aggregator)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.Adam(itertools.chain(model.parameters(), fea_para.parameters()), lr=0.01)\n",
    "    model.train()\n",
    "    \n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    loss_fun.to(device)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        print(\"************************************************************\")\n",
    "        for batch, (input_nodes, output_nodes, block) in enumerate(dataloader):\n",
    "        \n",
    "#             batch_feature = block[0].srcdata['features'] 这里也可以图省事不写load函数，感觉没啥问题\n",
    "#             batch_label = block[-1].dstdata['label']\n",
    "            batch_feature, batch_label = load_subtensor(nfeat, labels, output_nodes, input_nodes, device)\n",
    "            block = [block_.int().to(device) for block_ in block]\n",
    "            # block = [block_.to(device) for block_ in block]\n",
    "            model_pred = model(block, batch_feature)\n",
    "            loss = loss_fun(model_pred, batch_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch % 1 == 0:\n",
    "                print('Batch %d | Loss: %.4f' % (batch, loss.item()))\n",
    "        \n",
    "        # 验证一下模型的准确率\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"____________________________________________________________\")\n",
    "            val_acc = evaluate(model, my_net, labels, val_nid, val_mask, batch_s, num_worker, device)\n",
    "            train_acc = evaluate(model, my_net, labels, train_nid, train_mask, batch_s, num_worker, device)\n",
    "            print('Epoch %d | Val ACC: %.4f | Train ACC: %.4f' % (epoch, val_acc.item(), train_acc.item()))\n",
    "    \n",
    "    # 模型训练完毕，检查test集合的acc\n",
    "    acc_test = evaluate(model, my_net, labels, test_nid, test_mask, batch_s, num_worker, device)\n",
    "    print('Test ACC: %.4f' % (acc_test.item()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(node_fea):\n",
    "    \n",
    "    # 简单划分下训练集\n",
    "    train_node_ids = np.array(node_fea.groupby('label_number').apply(lambda x : x.sort_values('node_id_number')['node_id_number'].values[:20]))\n",
    "    val_node_ids = np.array(node_fea.groupby('label_number').apply(lambda x : x.sort_values('node_id_number')['node_id_number'].values[21:110]))\n",
    "    test_node_ids = np.array(node_fea.groupby('label_number').apply(lambda x : x.sort_values('node_id_number')['node_id_number'].values[111:300]))\n",
    "\n",
    "\n",
    "    train_nid = []\n",
    "    val_nid = []\n",
    "    test_nid = []\n",
    "    for (train_nodes, val_nodes, test_nodes) in zip(train_node_ids, val_node_ids, test_node_ids):\n",
    "        train_nid.extend(train_nodes)\n",
    "        val_nid.extend(val_nodes)\n",
    "        test_nid.extend(test_nodes)\n",
    "        \n",
    "    train_mask = node_fea['node_id_number'].apply(lambda x : x in train_nid)\n",
    "    val_mask = node_fea['node_id_number'].apply(lambda x : x in val_nid)\n",
    "    test_mask = node_fea['node_id_number'].apply(lambda x : x in test_nid)\n",
    "    \n",
    "    return train_mask, test_mask, val_mask, train_nid, test_nid, val_nid\n",
    "\n",
    "def loaddata():\n",
    "    node_fea = pd.read_table('/home/ray/code/python/GNN-from-Scratch/GNN/data/cora/cora.content', header=None)\n",
    "    edges = pd.read_table('/home/ray/code/python/GNN-from-Scratch/GNN/data/cora/cora.cites', header=None)\n",
    "    # 0是node id， 1434是node label\n",
    "    node_fea.rename(columns={0:'node_id', 1434:'label'}, inplace=True)\n",
    "\n",
    "    nodeID_number_dict = dict(zip(node_fea['node_id'].unique(), range(node_fea['node_id'].nunique())))\n",
    "    node_fea['node_id_number'] = node_fea['node_id'].map(nodeID_number_dict)\n",
    "    edges['edge1'] = edges[0].map(nodeID_number_dict)\n",
    "    edges['edge2'] = edges[1].map(nodeID_number_dict)\n",
    "\n",
    "    label_dict = dict(zip(node_fea['label'].unique(), range(node_fea['label'].nunique())))\n",
    "    node_fea['label_number'] = node_fea['label'].map(label_dict)\n",
    "    \n",
    "    src = np.array(edges['edge1'].values)\n",
    "    dst = np.array(edges['edge2'].values)\n",
    "\n",
    "    u = np.concatenate([src, dst])\n",
    "    v = np.concatenate([dst, src])\n",
    "\n",
    "    my_net = dgl.DGLGraph((u, v))\n",
    "\n",
    "    # 尝试第一种，用Embedding方法得到的相关\n",
    "    fea_id = range(1, 1434)\n",
    "    tensor_fea = torch.tensor(node_fea[fea_id].values, dtype=torch.float32)\n",
    "\n",
    "    fea_np = nn.Embedding(2708, 1433)\n",
    "    fea_np.weight = nn.Parameter(tensor_fea)\n",
    "\n",
    "#     my_net.ndata['features'] =  torch.tensor(node_fea[fea_id].values, dtype=torch.float32)\n",
    "    my_net.ndata['features'] = fea_np.weight\n",
    "    my_net.ndata['label'] = torch.tensor(node_fea['label_number'].values)\n",
    "    \n",
    "    in_feats = 1433\n",
    "    n_classes = node_fea['label'].nunique()\n",
    "    \n",
    "    data = in_feats, n_classes, my_net, fea_np\n",
    "    train_val_data = train_val_split(node_fea)\n",
    "    \n",
    "    return data, train_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/dataloading/dataloader.py:1149: DGLWarning: Dataloader CPU affinity opt is not enabled, consider switching it on (see enable_cpu_affinity() or CPU best practices for DGL [https://docs.dgl.ai/tutorials/cpu/cpu_best_practises.html])\n",
      "  dgl_warning(\n"
     ]
    },
    {
     "ename": "DGLError",
     "evalue": "[07:36:56] /opt/dgl/src/runtime/c_runtime_api.cc:82: Check failed: allow_missing: Device API cuda is not enabled. Please install the cuda version of dgl.\nStack trace:\n  [bt] (0) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x75) [0x7f063d33e8f5]\n  [bt] (1) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::DeviceAPIManager::GetAPI(std::string, bool)+0x202) [0x7f063d6ada92]\n  [bt] (2) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::DeviceAPI::Get(DGLContext, bool)+0x1e1) [0x7f063d6aa071]\n  [bt] (3) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::Empty(std::vector<long, std::allocator<long> >, DGLDataType, DGLContext)+0x13b) [0x7f063d6c554b]\n  [bt] (4) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::CopyTo(DGLContext const&) const+0xc3) [0x7f063d6ffd53]\n  [bt] (5) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::UnitGraph::CSR::CopyTo(DGLContext const&) const+0x1f0) [0x7f063d81db10]\n  [bt] (6) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::UnitGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DGLContext const&)+0xd1) [0x7f063d80cf21]\n  [bt] (7) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::HeteroGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DGLContext const&)+0xf6) [0x7f063d70c5d6]\n  [bt] (8) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(+0x51b396) [0x7f063d71b396]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m args \u001b[38;5;241m=\u001b[39m  hidden_size, n_layers, activation, dropout, aggregator, batch_s, num_worker  \n\u001b[0;32m---> 16\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_val_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(data, train_val_data, args, sample_size, learning_rate, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m batch_feature, batch_label \u001b[38;5;241m=\u001b[39m load_subtensor(nfeat, labels, output_nodes, input_nodes, device)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# block = [block_.int().to(device) for block_ in block]\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m block \u001b[38;5;241m=\u001b[39m [block_\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m block_ \u001b[38;5;129;01min\u001b[39;00m block]\n\u001b[1;32m     46\u001b[0m model_pred \u001b[38;5;241m=\u001b[39m model(block, batch_feature)\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fun(model_pred, batch_label)\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m batch_feature, batch_label \u001b[38;5;241m=\u001b[39m load_subtensor(nfeat, labels, output_nodes, input_nodes, device)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# block = [block_.int().to(device) for block_ in block]\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m block \u001b[38;5;241m=\u001b[39m [\u001b[43mblock_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m block_ \u001b[38;5;129;01min\u001b[39;00m block]\n\u001b[1;32m     46\u001b[0m model_pred \u001b[38;5;241m=\u001b[39m model(block, batch_feature)\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fun(model_pred, batch_label)\n",
      "File \u001b[0;32m~/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/heterograph.py:5709\u001b[0m, in \u001b[0;36mDGLGraph.to\u001b[0;34m(self, device, **kwargs)\u001b[0m\n\u001b[1;32m   5706\u001b[0m ret \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   5708\u001b[0m \u001b[38;5;66;03m# 1. Copy graph structure\u001b[39;00m\n\u001b[0;32m-> 5709\u001b[0m ret\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dgl_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5711\u001b[0m \u001b[38;5;66;03m# 2. Copy features\u001b[39;00m\n\u001b[1;32m   5712\u001b[0m \u001b[38;5;66;03m# TODO(minjie): handle initializer\u001b[39;00m\n\u001b[1;32m   5713\u001b[0m new_nframes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/heterograph_index.py:255\u001b[0m, in \u001b[0;36mHeteroGraphIndex.copy_to\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy_to\u001b[39m(\u001b[38;5;28mself\u001b[39m, ctx):\n\u001b[1;32m    241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Copy this immutable graph index to the given device context.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    NOTE: this method only works for immutable graph index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m        The graph index on the given device context.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_CAPI_DGLHeteroCopyTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mdgl/_ffi/_cython/./function.pxi:295\u001b[0m, in \u001b[0;36mdgl._ffi._cy3.core.FunctionBase.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mdgl/_ffi/_cython/./function.pxi:227\u001b[0m, in \u001b[0;36mdgl._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mdgl/_ffi/_cython/./function.pxi:217\u001b[0m, in \u001b[0;36mdgl._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDGLError\u001b[0m: [07:36:56] /opt/dgl/src/runtime/c_runtime_api.cc:82: Check failed: allow_missing: Device API cuda is not enabled. Please install the cuda version of dgl.\nStack trace:\n  [bt] (0) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x75) [0x7f063d33e8f5]\n  [bt] (1) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::DeviceAPIManager::GetAPI(std::string, bool)+0x202) [0x7f063d6ada92]\n  [bt] (2) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::DeviceAPI::Get(DGLContext, bool)+0x1e1) [0x7f063d6aa071]\n  [bt] (3) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::Empty(std::vector<long, std::allocator<long> >, DGLDataType, DGLContext)+0x13b) [0x7f063d6c554b]\n  [bt] (4) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::CopyTo(DGLContext const&) const+0xc3) [0x7f063d6ffd53]\n  [bt] (5) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::UnitGraph::CSR::CopyTo(DGLContext const&) const+0x1f0) [0x7f063d81db10]\n  [bt] (6) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::UnitGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DGLContext const&)+0xd1) [0x7f063d80cf21]\n  [bt] (7) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(dgl::HeteroGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DGLContext const&)+0xf6) [0x7f063d70c5d6]\n  [bt] (8) /home/ray/code/test/PDE/PeRCNN/percnn/lib/python3.8/site-packages/dgl/libdgl.so(+0x51b396) [0x7f063d71b396]\n\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "\n",
    "data, train_val_data = loaddata()\n",
    "\n",
    "hidden_size = 16\n",
    "n_layers = 2\n",
    "sample_size = [10, 25]\n",
    "activation = F.relu\n",
    "dropout = 0.5\n",
    "aggregator = 'mean'\n",
    "batch_s = 128\n",
    "num_worker = 0\n",
    "learning_rate = 0.003\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args =  hidden_size, n_layers, activation, dropout, aggregator, batch_s, num_worker  \n",
    "trained_model = run(data, train_val_data, args, sample_size, learning_rate, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "percnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
